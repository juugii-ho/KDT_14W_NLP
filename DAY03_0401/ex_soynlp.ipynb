{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Soynlp] 학습 기반 토크나이저\n",
    "- 품사 태깅, 단어 토큰화 등을 지원하는 단어 토크나이저\n",
    "- 비지도 학습으로 단어 토큰화 -> 데이터에 자주 등장하는 단어들을 단어로 분석\n",
    "- 내부적으로 단어 점수 표로 동작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['에이', '비식스', '이대', '휘', '1월', '최애', '돌', '기부', '요정', '입니다', '.']\n",
      "['에이', '비식스', '이대', '휘', '1월', '최애', '돌', '기부', '요정', '이다', '.']\n",
      "['에이', '비식스', '이대', '휘', '1월', '최애', '돌', '기부', '요정', '입니다', '.']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Okt()\n",
    "\n",
    "print(tokenizer.morphs(\"에이비식스 이대휘 1월 최애돌 기부 요정 입니다.\"))\n",
    "\n",
    "# 형태소 분석 시 매개변수 stem, norm 둘 다 default는 False \n",
    "print(tokenizer.morphs(\"에이비식스 이대휘 1월 최애돌 기부 요정 입니다.\", stem = True)) # 어간\n",
    "print(tokenizer.morphs(\"에이비식스 이대휘 1월 최애돌 기부 요정 입니다.\", norm = True)) # 정규화 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [soynlp] 사용 -> 말뭉치 데이터셋 \n",
    "from urllib.request import urlretrieve\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../data/text_data.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ===> 학습 데이터 처리\n",
    "from soynlp import DoublespaceLineCorpus # 한개로 통합된 문서 데이터를 분리하기 위함 \n",
    "from soynlp.word import WordExtractor # 단어 추출 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터 문서 : 30091개\n"
     ]
    }
   ],
   "source": [
    "### ===> 훈련 데이터 문서 분리\n",
    "corpus = DoublespaceLineCorpus(filename)\n",
    "print(f\"훈련 데이터 문서 : {len(corpus)}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.196 Gb\n",
      "all cohesion probabilities was computed. # words = 223348\n",
      "all branching entropies was computed # words = 361598\n",
      "all accessor variety was computed # words = 361598\n"
     ]
    }
   ],
   "source": [
    "# [주의] 실행 시 오래걸려서 주석으로 바꿔둠\n",
    "\n",
    "### ===> SoyNLP 학습 진행\n",
    "word_extractor = WordExtractor()\n",
    "\n",
    "# 학습 진행하며 단어별 점수\n",
    "word_extractor.train(sents=corpus)\n",
    "\n",
    "# 단어별 점수표 추출\n",
    "word_score_table = word_extractor.extract()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]-길\n",
      "[1]-걷\n",
      "[2]-애\n",
      "[3]-알\n",
      "[4]-팁\n",
      "[5]-덩\n",
      "[6]-옴\n",
      "[7]-쿄\n",
      "[8]-꼴\n",
      "[9]-참\n",
      "[10]-챌\n",
      "[11]-낸\n",
      "[12]-워\n",
      "[13]-던\n",
      "[14]-벅\n",
      "[15]-넬\n",
      "[16]-팅\n",
      "[17]-찬\n",
      "[18]-뇌\n",
      "[19]-래\n",
      "[20]-몸\n",
      "[21]-덤\n",
      "[22]-복\n",
      "[23]-농\n",
      "[24]-솜\n",
      "[25]-깼\n",
      "[26]-같\n",
      "[27]-뺏\n",
      "[28]-덜\n",
      "[29]-핼\n",
      "[30]-심\n"
     ]
    }
   ],
   "source": [
    "# 단어별 점수표 확인\n",
    "for idx, key in enumerate(iterable=word_score_table):\n",
    "    print(f'[{idx}]-{key}')\n",
    "    if idx == 30:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ===> 응집 확률(cohesion probablity) : 내부 문자열(substring)이 얼마나 응집하여 자주 등장하는지를 판단하는 척도\n",
    "# - 원리 : 문자열을 문자 단위로 분리, 왼쪽부터 순서대로 문자를 추가\n",
    "#         각 문자열이 주어졌을 때 그 다음 문자가 나올 확률을 계산 / 누적곱 한 값\n",
    "# - 값이 높을 수록 : 전체 코퍼스에서 이 문자열 시퀀스는 하나의 단어로 등장할 가능성 높음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_score_table['바'].cohesion_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06393648140409527"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_score_table['바다'].cohesion_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11518621707955429"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_score_table['바다에'].cohesion_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ===> SOYNLP의 L tokenzer\n",
    "# - 띄어쓰기 단위로 나누 어절 토큰 : L 토큰 + R 토큰\n",
    "#   (예 : '공원에' => '공원' + '에', '공부하는' => '공부 + 하는')\n",
    "# - 분리 기준 : 점수가 가장 높은 L 토큰을 찾아내는 원리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('국제사회', '와'), ('우리', '의'), ('노력', '들로'), ('범죄', '를'), ('척결', '하자')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from soynlp.tokenizer import LTokenizer\n",
    "\n",
    "# 토큰으로 쪼개기 위한 L토큰 \n",
    "scores = {word:score.cohesion_forward for word, score in word_score_table.items()}\n",
    "\n",
    "l_tokenizer = LTokenizer(scores = scores)\n",
    "l_tokenizer.tokenize('국제사회와 우리의 노력들로 범죄를 척결하자', flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['국제사회', '와', '우리', '의', '노력', '들로', '범죄', '를', '척결', '하자']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### ===> 최대 점수 토크나이저\n",
    "# - 띄어쓰기가 되지 않는 문장에서 점수가 높은 글자 시퀀스를 순차적으로 찾아내는 토크나이저\n",
    "# - 띄어쓰기가 되어 있지 않은 묹아을 넣어서 점수를 통해 토큰화 된 결과\n",
    "\n",
    "from soynlp.tokenizer import MaxScoreTokenizer\n",
    "\n",
    "maxscore_tokenizer = MaxScoreTokenizer(scores=scores)       # MaxScoreTokenizer는 학습된 데이터셋을 바탕으로 하는 것\n",
    "maxscore_tokenizer.tokenize('국제사회와우리의노력들로범죄를척결하자')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SOYNLP를 이용한 반복되는 문자 정제\n",
    "# - ㅋㅋ, ㅎㅎ 등의 이모티콘의 경우 불필요하게 연속되는 경우 많음\n",
    "# - ㅋㅋ, ㅋㅋㅋ, ㅋㅋㅋㅋ와 같은 경우를 모두 서로 다른 단어로 처리하는 것은 불필요\n",
    "# >>> 반복되는 것은 하나로 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아ㅋ화존잼쓰ㅠ\n",
      "아ㅋㅋ화존잼쓰ㅠㅠ\n",
      "아ㅋㅋ화존잼쓰ㅠㅠ\n",
      "아ㅋㅋ화존잼쓰ㅠㅠ\n",
      "아ㅋㅋㅋ화존잼쓰ㅠㅠㅠ\n"
     ]
    }
   ],
   "source": [
    "from soynlp.normalizer import *\n",
    "\n",
    "print(emoticon_normalize('앜ㅋㅋㅋ영화존잼쓰ㅠㅠㅠㅠ', num_repeats=1))\n",
    "print(emoticon_normalize('앜ㅋㅋㅋ영화존잼쓰ㅠㅠㅠㅠ', num_repeats=2))\n",
    "print(emoticon_normalize('앜ㅋㅋㅋㅋ영화존잼쓰ㅠㅠㅠㅠㅠ', num_repeats=2))\n",
    "print(emoticon_normalize('앜ㅋㅋㅋㅋㅋ영화존잼쓰ㅠㅠㅠㅠㅠㅠ', num_repeats=2))\n",
    "print(emoticon_normalize('앜ㅋㅋㅋㅋㅋㅋ영화존잼쓰ㅠㅠㅠㅠㅠㅠㅠ', num_repeats=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "와하하핫\n",
      "와하하핫\n",
      "와하하핫\n",
      "와하하하핫\n",
      "와하하핫\n",
      "와하핫\n",
      "앜^^^^^^^^^^^^^^^^^^\n"
     ]
    }
   ],
   "source": [
    "print(repeat_normalize('와하하하하하하하하핫', num_repeats=2))\n",
    "print(repeat_normalize('와하하하하하하핫', num_repeats=2))\n",
    "print(repeat_normalize('와하하하하핫', num_repeats=2))\n",
    "print(repeat_normalize('와하하하핫', num_repeats=2))\n",
    "print(repeat_normalize('와하하핫', num_repeats=2))\n",
    "print(repeat_normalize('와하핫', num_repeats=2))\n",
    "\n",
    "print(repeat_normalize('앜^^^^^^^^^^^^^^^^^^', num_repeats=1))            # ^는 이모티콘으로 인식하지 않음, 구두점에서 제거해야\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sml/anaconda3/envs/Torch_NLP38/lib/python3.8/site-packages/konlpy/tag/_okt.py:17: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['은', '경이', '는', '사무실', '로', '갔습니다', '.']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ckonlpy.tag import Twitter\n",
    "\n",
    "twitter = Twitter()\n",
    "twitter.morphs('은경이는 사무실로 갔습니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 분석기에 사전 추가\n",
    "twitter.add_dictionary('은경이','Noun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['은경이', '는', '사무실', '로', '갔습니다', '.']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter.morphs('은경이는 사무실로 갔습니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch_NLP38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
