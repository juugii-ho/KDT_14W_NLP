{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [ 순환신경망 RNN ] <hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### ===> 데이터 및 초기 Hidden State\n",
    "\n",
    "# input = torch.randn(1, 3, 10)       # 입력 데이터 (배치크기, 시퀀스길이, 피쳐길이)\n",
    "# h0 = torch.randn(2, 3, 1)           # 3이 배치사이즈, 1이 히든 초기값\n",
    "\n",
    "# ### ===> RNN 인스턴스 생성\n",
    "# rnn = nn.RNN(10, 1, 1)              # 피쳐길이, 히든사이즈, 레이어 수 \n",
    "\n",
    "# output, hn = rnn(input, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### -------------------------------------------\n",
    "### ===> 설계 : 다층 RNN, 층 2개\n",
    "### ===> INPUT\n",
    "### -------------------------------------------\n",
    "### 입력 초기 텐서들\n",
    "input = torch.randn(1, 3, 10)       # 배치사이즈, 시퀀스(1개문장구성 단어 수), 피쳐 수(1개 단어 표현하는 수)\n",
    "h0 = torch.randn(1, 3, 1)           # 히든스테이트 초기화(양방향*층수, 배치크기, 히든갯수)\n",
    "### RNN 인스턴스\n",
    "rnn = nn.RNN(10, 1, 1)\n",
    "\n",
    "### 출력 텐서들\n",
    "output, hn = rnn(input, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 1\n",
    "NUM_LAYERS = 1\n",
    "SEQ_LENGTH = 3\n",
    "BATCH_SIZE = 1\n",
    "BIDIRECTIONAL = 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 데이터 및 초기 Hidden state\n",
    "input = torch.randn(BATCH_SIZE, SEQ_LENGTH, 10)                               # 배치사이즈, 시퀀스(1개문장구성 단어 수), 피쳐 수(1개 단어 표현하는 수)\n",
    "h0 = torch.randn(NUM_LAYERS*BIDIRECTIONAL, BATCH_SIZE, HIDDEN_SIZE)           # 히든스테이트 초기화(양방향*층수, 배치크기, 히든갯수)\n",
    "### RNN 인스턴스\n",
    "rnn = nn.RNN(10, HIDDEN_SIZE, NUM_LAYERS, batch_first=True)\n",
    "\n",
    "### 출력 텐서들\n",
    "output, hn = rnn(input, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ INPUT DATA ]\n",
      "-Shape : torch.Size([1, 3, 10])     DIM : 3D\n"
     ]
    }
   ],
   "source": [
    "print(f'[ INPUT DATA ]\\n-Shape : {input.shape}     DIM : {input.ndim}D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ RNN 출력 - output ]\n",
      "-Shape : torch.Size([1, 3, 1])   DIM : 3D\n"
     ]
    }
   ],
   "source": [
    "print(f'[ RNN 출력 - output ]\\n-Shape : {output.shape}   DIM : {output.ndim}D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ RNN 출력 - hn ]\n",
      "-Shape : torch.Size([1, 1, 1])   DIM : 3D\n"
     ]
    }
   ],
   "source": [
    "print(f'[ RNN 출력 - hn ]\\n-Shape : {hn.shape}   DIM : {hn.ndim}D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ RNN PARAMETERS]\n",
      "-----[weight_ih_l0]\n",
      "Parameter containing:\n",
      "tensor([[ 0.5070,  0.9193,  0.3934,  0.0720,  0.7885, -0.9569, -0.0319, -0.5168,\n",
      "         -0.4571,  0.0134]], requires_grad=True)\n",
      "\n",
      "-----[weight_hh_l0]\n",
      "Parameter containing:\n",
      "tensor([[0.5485]], requires_grad=True)\n",
      "\n",
      "-----[bias_ih_l0]\n",
      "Parameter containing:\n",
      "tensor([0.2836], requires_grad=True)\n",
      "\n",
      "-----[bias_hh_l0]\n",
      "Parameter containing:\n",
      "tensor([-0.2647], requires_grad=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'[ RNN PARAMETERS]')\n",
    "for name, param in rnn.named_parameters():\n",
    "    print(f'-----[{name}]\\n{param}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[all_weights] - [[Parameter containing:\n",
      "tensor([[ 0.5070,  0.9193,  0.3934,  0.0720,  0.7885, -0.9569, -0.0319, -0.5168,\n",
      "         -0.4571,  0.0134]], requires_grad=True), Parameter containing:\n",
      "tensor([[0.5485]], requires_grad=True), Parameter containing:\n",
      "tensor([0.2836], requires_grad=True), Parameter containing:\n",
      "tensor([-0.2647], requires_grad=True)]], 1개\n",
      " =====> [weight_ih_l0]\n",
      "Parameter containing:\n",
      "tensor([[ 0.5070,  0.9193,  0.3934,  0.0720,  0.7885, -0.9569, -0.0319, -0.5168,\n",
      "         -0.4571,  0.0134]], requires_grad=True)\n",
      ")\n",
      " =====> [weight_hh_l0]\n",
      "Parameter containing:\n",
      "tensor([[0.5485]], requires_grad=True)\n",
      ")\n",
      " =====> [bias_ih_l0]\n",
      "Parameter containing:\n",
      "tensor([0.2836], requires_grad=True)\n",
      ")\n",
      " =====> [bias_hh_l0]\n",
      "Parameter containing:\n",
      "tensor([-0.2647], requires_grad=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "### RNN 모델의 속성 출력\n",
    "print(f'[all_weights] - {rnn.all_weights}, {len(rnn.all_weights)}개')\n",
    "\n",
    "for name, param in rnn.named_parameters():\n",
    "    print(f' =====> [{name}]\\n{param}\\n)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "RNN                                      13\n",
       "=================================================================\n",
       "Total params: 13\n",
       "Trainable params: 13\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 1]),\n",
       " 3,\n",
       " tensor([[[0.9778],\n",
       "          [0.9799],\n",
       "          [0.9716]]], grad_fn=<TransposeBackward1>))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### ===> RNN 출력 텐서 output\n",
    "output.shape, output.ndim, output  # (배치사이즈, 시퀀스, 히든스테이트)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2245231627.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[12], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    1. 단어 임베딩(Word Embedding)\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "1. 단어 임베딩(Word Embedding)\n",
    "- 단어 사전: {\"i\": [1, 0, 0], \"am\": [0, 1, 0], \"here\": [0, 0, 1]}\n",
    "\n",
    "2. 초기 은닉 상태(Initial Hidden State)\n",
    "- h0 = [0, 0] (모든 값을 0으로 초기화)\n",
    "\n",
    "3. 시간 단계 t=1 (단어 \"i\")\n",
    "- x1 = [1, 0, 0] (단어 \"i\"의 원핫 벡터)\n",
    "- h1 = tanh(W_hx * x1 + W_hh * h0 + b_h)\n",
    "     = tanh([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]] * [1, 0, 0] + [[0.7, 0.8], [0.9, 1.0]] * [0, 0] + [0.1, 0.2])\n",
    "     = tanh([0.1, 0.2, 0.5, 0.6] + [0.1, 0.2])\n",
    "     = tanh([0.2, 0.4, 0.6, 0.8])\n",
    "     = [0.197, 0.378]\n",
    "\n",
    "4. 시간 단계 t=2 (단어 \"am\")\n",
    "- x2 = [0, 1, 0] (단어 \"am\"의 원핫 벡터)\n",
    "- h2 = tanh(W_hx * x2 + W_hh * h1 + b_h)\n",
    "     = tanh([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]] * [0, 1, 0] + [[0.7, 0.8], [0.9, 1.0]] * [0.197, 0.378] + [0.1, 0.2])\n",
    "     = tanh([0.3, 0.4, 0.5, 0.6] + [0.438, 0.578])\n",
    "     = tanh([0.738, 0.978, 0.5, 0.6])\n",
    "     = [0.593, 0.726, 0.462, 0.544]\n",
    "\n",
    "5. 시간 단계 t=3 (단어 \"here\")\n",
    "- x3 = [0, 0, 1] (단어 \"here\"의 원핫 벡터)\n",
    "- h3 = tanh(W_hx * x3 + W_hh * h2 + b_h)\n",
    "     = tanh([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]] * [0, 0, 1] + [[0.7, 0.8], [0.9, 1.0]] * [0.593, 0.726] + [0.1, 0.2])\n",
    "     = tanh([0.5, 0.6] + [0.915, 1.126])\n",
    "     = tanh([1.415, 1.726])\n",
    "     = [0.802, 0.851]\n",
    "\n",
    "최종 은닉 상태 h3 = [0.802, 0.851]는 RNN의 출력이 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "네, 원핫 인코딩된 입력부터 계산 과정을 보여드리겠습니다.\n",
    "\n",
    "1. 입력 시퍼스: [i, am, here, now]\n",
    "\n",
    "2. 원핫 인코딩\n",
    "- i: [1, 0, 0, 0]\n",
    "- am: [0, 1, 0, 0]\n",
    "- here: [0, 0, 1, 0]\n",
    "- now: [0, 0, 0, 1]\n",
    "\n",
    "3. 초기 은닉 상태 \n",
    "- h0 = [0, 0] (모든 값을 0으로 초기화)\n",
    "\n",
    "4. 시간 단계 t=1 (단어 \"i\")\n",
    "- x1 = [1, 0, 0, 0]\n",
    "- h1 = tanh(W_hx * x1 + W_hh * h0 + b_h)\n",
    "     = tanh([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]] * [1, 0, 0, 0] + [[0.9, 1.0], [1.1, 1.2]] * [0, 0] + [0.1, 0.2])\n",
    "     = tanh([0.1, 0.2, 0.3, 0.4] + [0.1, 0.2])\n",
    "     = tanh([0.2, 0.4, 0.3, 0.4])\n",
    "     = [0.197, 0.378, 0.296, 0.378]\n",
    "\n",
    "5. 시간 단계 t=2 (단어 \"am\")\n",
    "- x2 = [0, 1, 0, 0]  \n",
    "- h2 = tanh(W_hx * x2 + W_hh * h1 + b_h)\n",
    "     = tanh([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]] * [0, 1, 0, 0] + [[0.9, 1.0], [1.1, 1.2]] * [0.197, 0.378, 0.296, 0.378] + [0.1, 0.2])\n",
    "     = tanh([0.5, 0.6, 0.7, 0.8] + [0.477, 0.734, 0.522, 0.734])\n",
    "     = tanh([0.977, 1.334, 1.222, 1.534])\n",
    "     = [0.728, 0.863, 0.828, 0.907]\n",
    "\n",
    "6. 시간 단계 t=3 (단어 \"here\")\n",
    "- x3 = [0, 0, 1, 0]\n",
    "- h3 = tanh(W_hx * x3 + W_hh * h2 + b_h)\n",
    "     = tanh([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]] * [0, 0, 1, 0] + [[0.9, 1.0], [1.1, 1.2]] * [0.728, 0.863, 0.828, 0.907] + [0.1, 0.2])\n",
    "     = tanh([0.3, 0.4, 0.7, 0.8] + [1.255, 1.603, 1.418, 1.794])\n",
    "     = tanh([1.555, 2.003, 2.118, 2.594])\n",
    "     = [0.906, 0.951, 0.958, 0.979]\n",
    "\n",
    "7. 시간 단계 t=4 (단어 \"now\")\n",
    "- x4 = [0, 0, 0, 1]\n",
    "- h4 = tanh(W_hx * x4 + W_hh * h3 + b_h)\n",
    "     = tanh([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]] * [0, 0, 0, 1] + [[0.9, 1.0], [1.1, 1.2]] * [0.906, 0.951, 0.958, 0.979] + [0.1, 0.2])\n",
    "     = tanh([0.4, 0.8] + [1.897, 2.088])\n",
    "     = tanh([2.297, 2.888])\n",
    "     = [0.969, 0.989]\n",
    "\n",
    "최종 은닉 상태 h4 = [0.969, 0.989]가 RNN의 출력이 됩니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch_NLP38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
